INFO:WNN:Epoch 0: Training Loss 0.35077071176945335 	 Validation Loss 0.22450130805373192
INFO:WNN:Epoch 1: Training Loss 0.2771870329133457 	 Validation Loss 0.20086142048239708
INFO:WNN:Epoch 2: Training Loss 0.12840664269225227 	 Validation Loss 0.16096916235983372
INFO:WNN:Epoch 3: Training Loss 0.17113452867626966 	 Validation Loss 0.17249516630545259
INFO:WNN:Epoch 4: Training Loss 0.30641940828100483 	 Validation Loss 0.5876961424946785
INFO:WNN:Epoch 5: Training Loss 0.3111259383334231 	 Validation Loss 0.3708494072780013
INFO:WNN:Epoch 6: Training Loss 0.22961106696230774 	 Validation Loss 0.14503042958676815
INFO:WNN:Epoch 7: Training Loss 0.14555370259109796 	 Validation Loss 0.055094640236347914
INFO:WNN:Epoch 8: Training Loss 0.3615780598040493 	 Validation Loss 0.33432733453810215
INFO:WNN:Epoch 9: Training Loss 0.5802843335993766 	 Validation Loss 0.26683926582336426
INFO:WNN:Epoch 10: Training Loss 0.5107073877819829 	 Validation Loss 0.1448233462870121
INFO:WNN:Epoch 11: Training Loss 0.534782591126194 	 Validation Loss 0.3228985220193863
INFO:WNN:Epoch 12: Training Loss 0.6409329013965491 	 Validation Loss 0.6789205893874168
INFO:WNN:Epoch 13: Training Loss 0.7389811053373185 	 Validation Loss 0.30302394554018974
INFO:WNN:Epoch 14: Training Loss 0.40012754378322923 	 Validation Loss 0.15032939426600933
INFO:WNN:Epoch 15: Training Loss 0.2485610092472699 	 Validation Loss 0.10892474558204412
INFO:WNN:Epoch 16: Training Loss 0.19719382091369925 	 Validation Loss 0.12684378772974014
INFO:WNN:Epoch 17: Training Loss 0.24913909813640492 	 Validation Loss 0.204122856259346
INFO:WNN:Epoch 18: Training Loss 0.2523969037145088 	 Validation Loss 0.10012956219725311
threshold tensor(0.7840, grad_fn=<MulBackward0>)
