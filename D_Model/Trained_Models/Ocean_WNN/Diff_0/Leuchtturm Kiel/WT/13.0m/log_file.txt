INFO:WNN:Epoch 0: Training Loss 0.49029491618275645 	 Validation Loss 0.6407291293144226
INFO:WNN:Epoch 1: Training Loss 0.2814126193523407 	 Validation Loss 0.32549458742141724
INFO:WNN:Epoch 2: Training Loss 0.11272066459059715 	 Validation Loss 0.29943278431892395
INFO:WNN:Epoch 3: Training Loss 0.16074489653110505 	 Validation Loss 0.40518835186958313
INFO:WNN:Epoch 4: Training Loss 0.07195706777274609 	 Validation Loss 0.6817095279693604
INFO:WNN:Epoch 5: Training Loss 0.09414027109742165 	 Validation Loss 0.7890658378601074
INFO:WNN:Epoch 6: Training Loss 0.06953504160046578 	 Validation Loss 0.5809743404388428
INFO:WNN:Epoch 7: Training Loss 0.06174848601222038 	 Validation Loss 0.3978685438632965
INFO:WNN:Epoch 8: Training Loss 0.06532667074352502 	 Validation Loss 0.3568427264690399
INFO:WNN:Epoch 9: Training Loss 0.05669887475669384 	 Validation Loss 0.3790486454963684
INFO:WNN:Epoch 10: Training Loss 0.05644333865493536 	 Validation Loss 0.37587663531303406
INFO:WNN:Epoch 11: Training Loss 0.05207784008234739 	 Validation Loss 0.2992880642414093
INFO:WNN:Epoch 12: Training Loss 0.05094489715993404 	 Validation Loss 0.24347637593746185
INFO:WNN:Epoch 13: Training Loss 0.049642302468419074 	 Validation Loss 0.23873719573020935
INFO:WNN:Epoch 14: Training Loss 0.04864111877977848 	 Validation Loss 0.24345344305038452
INFO:WNN:Epoch 15: Training Loss 0.047303823940455916 	 Validation Loss 0.2211984097957611
INFO:WNN:Epoch 16: Training Loss 0.04628826286643743 	 Validation Loss 0.20314735174179077
INFO:WNN:Epoch 17: Training Loss 0.04565800186246634 	 Validation Loss 0.1986638605594635
INFO:WNN:Epoch 18: Training Loss 0.044963687658309937 	 Validation Loss 0.19681869447231293
INFO:WNN:Epoch 19: Training Loss 0.04418572373688221 	 Validation Loss 0.19331473112106323
INFO:WNN:Epoch 20: Training Loss 0.043479323759675025 	 Validation Loss 0.18461905419826508
INFO:WNN:Epoch 21: Training Loss 0.042928978055715564 	 Validation Loss 0.18201957643032074
INFO:WNN:Epoch 22: Training Loss 0.04234161525964737 	 Validation Loss 0.1827426552772522
INFO:WNN:Epoch 23: Training Loss 0.04176263082772493 	 Validation Loss 0.17777739465236664
INFO:WNN:Epoch 24: Training Loss 0.04123362526297569 	 Validation Loss 0.17562174797058105
INFO:WNN:Epoch 25: Training Loss 0.04076443817466498 	 Validation Loss 0.17448736727237701
INFO:WNN:Epoch 26: Training Loss 0.04027942158281803 	 Validation Loss 0.1732301563024521
INFO:WNN:Epoch 27: Training Loss 0.03981671240180731 	 Validation Loss 0.17146846652030945
INFO:WNN:Epoch 28: Training Loss 0.03939410336315632 	 Validation Loss 0.170713409781456
INFO:WNN:Epoch 29: Training Loss 0.03896877523511648 	 Validation Loss 0.17089274525642395
INFO:WNN:Epoch 30: Training Loss 0.03856144044548273 	 Validation Loss 0.17040996253490448
INFO:WNN:Epoch 31: Training Loss 0.03816601280122996 	 Validation Loss 0.17140813171863556
INFO:WNN:Epoch 32: Training Loss 0.03778679091483354 	 Validation Loss 0.17264582216739655
INFO:WNN:Epoch 33: Training Loss 0.037406219728291036 	 Validation Loss 0.17480994760990143
INFO:WNN:Epoch 34: Training Loss 0.03704707734286785 	 Validation Loss 0.1774280071258545
INFO:WNN:Epoch 35: Training Loss 0.03668469097465277 	 Validation Loss 0.18147708475589752
INFO:WNN:Epoch 36: Training Loss 0.03633710779249668 	 Validation Loss 0.18574926257133484
threshold tensor(0.7049, grad_fn=<MulBackward0>)
