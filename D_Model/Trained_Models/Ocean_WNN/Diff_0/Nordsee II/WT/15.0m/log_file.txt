INFO:WNN:Epoch 0: Training Loss 0.9066362082958221 	 Validation Loss 2.0549066066741943
INFO:WNN:Epoch 1: Training Loss 0.6367664709687233 	 Validation Loss 1.856488585472107
INFO:WNN:Epoch 2: Training Loss 0.5155589357018471 	 Validation Loss 1.666905164718628
INFO:WNN:Epoch 3: Training Loss 0.42851803451776505 	 Validation Loss 1.6093000173568726
INFO:WNN:Epoch 4: Training Loss 0.4226484149694443 	 Validation Loss 1.6225751638412476
INFO:WNN:Epoch 5: Training Loss 0.4003543257713318 	 Validation Loss 1.5379680395126343
INFO:WNN:Epoch 6: Training Loss 0.3817720077931881 	 Validation Loss 1.4558579921722412
INFO:WNN:Epoch 7: Training Loss 0.3765091374516487 	 Validation Loss 1.4044866561889648
INFO:WNN:Epoch 8: Training Loss 0.3599765673279762 	 Validation Loss 1.3800264596939087
INFO:WNN:Epoch 9: Training Loss 0.35135839134454727 	 Validation Loss 1.3515592813491821
INFO:WNN:Epoch 10: Training Loss 0.3411073237657547 	 Validation Loss 1.3135652542114258
INFO:WNN:Epoch 11: Training Loss 0.3329449016600847 	 Validation Loss 1.293705940246582
INFO:WNN:Epoch 12: Training Loss 0.32655428536236286 	 Validation Loss 1.288568139076233
INFO:WNN:Epoch 13: Training Loss 0.31981629319489 	 Validation Loss 1.2824281454086304
INFO:WNN:Epoch 14: Training Loss 0.3123032432049513 	 Validation Loss 1.2725123167037964
INFO:WNN:Epoch 15: Training Loss 0.30502299033105373 	 Validation Loss 1.268720030784607
INFO:WNN:Epoch 16: Training Loss 0.29634745325893164 	 Validation Loss 1.269256353378296
INFO:WNN:Epoch 17: Training Loss 0.2870230386033654 	 Validation Loss 1.2682216167449951
INFO:WNN:Epoch 18: Training Loss 0.27653131261467934 	 Validation Loss 1.2683088779449463
INFO:WNN:Epoch 19: Training Loss 0.26536518801003695 	 Validation Loss 1.2715405225753784
INFO:WNN:Epoch 20: Training Loss 0.2532801874913275 	 Validation Loss 1.276065707206726
INFO:WNN:Epoch 21: Training Loss 0.24130764044821262 	 Validation Loss 1.2844830751419067
INFO:WNN:Epoch 22: Training Loss 0.22929396270774305 	 Validation Loss 1.2974382638931274
threshold tensor(9.8976, grad_fn=<MulBackward0>)
