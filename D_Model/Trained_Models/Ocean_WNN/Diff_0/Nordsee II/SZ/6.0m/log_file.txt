INFO:WNN:Epoch 0: Training Loss 0.8153649841745695 	 Validation Loss 0.4865872263908386
INFO:WNN:Epoch 1: Training Loss 0.36689332127571106 	 Validation Loss 0.44690167903900146
INFO:WNN:Epoch 2: Training Loss 0.310991903146108 	 Validation Loss 0.49655207991600037
INFO:WNN:Epoch 3: Training Loss 0.2332739649961392 	 Validation Loss 0.519489586353302
INFO:WNN:Epoch 4: Training Loss 0.21714322113742432 	 Validation Loss 0.4272814393043518
INFO:WNN:Epoch 5: Training Loss 0.19056331117947897 	 Validation Loss 0.3063086271286011
INFO:WNN:Epoch 6: Training Loss 0.17252562195062637 	 Validation Loss 0.24526795744895935
INFO:WNN:Epoch 7: Training Loss 0.14854449778795242 	 Validation Loss 0.24841545522212982
INFO:WNN:Epoch 8: Training Loss 0.11873386800289154 	 Validation Loss 0.29533734917640686
INFO:WNN:Epoch 9: Training Loss 0.11843537104626496 	 Validation Loss 0.31850725412368774
INFO:WNN:Epoch 10: Training Loss 0.11661570519208908 	 Validation Loss 0.27717018127441406
INFO:WNN:Epoch 11: Training Loss 0.10959376208484173 	 Validation Loss 0.2273670881986618
INFO:WNN:Epoch 12: Training Loss 0.1051537146170934 	 Validation Loss 0.20543910562992096
INFO:WNN:Epoch 13: Training Loss 0.09643700160086155 	 Validation Loss 0.197567880153656
INFO:WNN:Epoch 14: Training Loss 0.09439315150181453 	 Validation Loss 0.1875646412372589
INFO:WNN:Epoch 15: Training Loss 0.0898852776736021 	 Validation Loss 0.18093952536582947
INFO:WNN:Epoch 16: Training Loss 0.08450598642230034 	 Validation Loss 0.18329788744449615
INFO:WNN:Epoch 17: Training Loss 0.08093786394844453 	 Validation Loss 0.1870567798614502
INFO:WNN:Epoch 18: Training Loss 0.07960584588969748 	 Validation Loss 0.18322032690048218
INFO:WNN:Epoch 19: Training Loss 0.07730009686201811 	 Validation Loss 0.1766585409641266
INFO:WNN:Epoch 20: Training Loss 0.07552556755642097 	 Validation Loss 0.17374013364315033
INFO:WNN:Epoch 21: Training Loss 0.073594456538558 	 Validation Loss 0.17299993336200714
INFO:WNN:Epoch 22: Training Loss 0.07261050445958972 	 Validation Loss 0.17319728434085846
INFO:WNN:Epoch 23: Training Loss 0.07127542824794848 	 Validation Loss 0.17363448441028595
INFO:WNN:Epoch 24: Training Loss 0.07026148051954806 	 Validation Loss 0.17290645837783813
INFO:WNN:Epoch 25: Training Loss 0.06943897254920255 	 Validation Loss 0.17131352424621582
INFO:WNN:Epoch 26: Training Loss 0.0685986093012616 	 Validation Loss 0.1701749712228775
INFO:WNN:Epoch 27: Training Loss 0.06788821844384074 	 Validation Loss 0.16897203028202057
INFO:WNN:Epoch 28: Training Loss 0.0671067318180576 	 Validation Loss 0.16750170290470123
INFO:WNN:Epoch 29: Training Loss 0.06645628156062837 	 Validation Loss 0.16632932424545288
INFO:WNN:Epoch 30: Training Loss 0.06580392009345815 	 Validation Loss 0.16455747187137604
INFO:WNN:Epoch 31: Training Loss 0.06518590962514281 	 Validation Loss 0.1619362235069275
INFO:WNN:Epoch 32: Training Loss 0.06460568867623806 	 Validation Loss 0.15942911803722382
INFO:WNN:Epoch 33: Training Loss 0.0640433927377065 	 Validation Loss 0.15709029138088226
INFO:WNN:Epoch 34: Training Loss 0.06344698590692133 	 Validation Loss 0.15468862652778625
INFO:WNN:Epoch 35: Training Loss 0.0629114875337109 	 Validation Loss 0.1524684578180313
INFO:WNN:Epoch 36: Training Loss 0.062376062269322574 	 Validation Loss 0.15025269985198975
INFO:WNN:Epoch 37: Training Loss 0.061835147867289685 	 Validation Loss 0.14804217219352722
INFO:WNN:Epoch 38: Training Loss 0.06132202052200834 	 Validation Loss 0.14621597528457642
INFO:WNN:Epoch 39: Training Loss 0.060806927465212844 	 Validation Loss 0.14466261863708496
INFO:WNN:Epoch 40: Training Loss 0.06028957094531506 	 Validation Loss 0.14317649602890015
INFO:WNN:Epoch 41: Training Loss 0.05979872265985856 	 Validation Loss 0.14176611602306366
INFO:WNN:Epoch 42: Training Loss 0.05930131227554133 	 Validation Loss 0.1403798758983612
INFO:WNN:Epoch 43: Training Loss 0.058805249786625303 	 Validation Loss 0.13911330699920654
INFO:WNN:Epoch 44: Training Loss 0.05832156395384421 	 Validation Loss 0.13801749050617218
INFO:WNN:Epoch 45: Training Loss 0.057830838447747134 	 Validation Loss 0.13696017861366272
INFO:WNN:Epoch 46: Training Loss 0.057350587487841644 	 Validation Loss 0.1359407901763916
INFO:WNN:Epoch 47: Training Loss 0.056874691896761455 	 Validation Loss 0.13499866425991058
INFO:WNN:Epoch 48: Training Loss 0.05639686276360104 	 Validation Loss 0.13415658473968506
INFO:WNN:Epoch 49: Training Loss 0.05592915524418155 	 Validation Loss 0.13341523706912994
INFO:WNN:Epoch 50: Training Loss 0.05546169347750644 	 Validation Loss 0.13270872831344604
INFO:WNN:Epoch 51: Training Loss 0.0549991235214596 	 Validation Loss 0.1320452243089676
INFO:WNN:Epoch 52: Training Loss 0.05454140901565552 	 Validation Loss 0.13146790862083435
INFO:WNN:Epoch 53: Training Loss 0.05408330843783915 	 Validation Loss 0.13098761439323425
INFO:WNN:Epoch 54: Training Loss 0.05363205762114376 	 Validation Loss 0.13059112429618835
INFO:WNN:Epoch 55: Training Loss 0.05318240382863829 	 Validation Loss 0.13027027249336243
INFO:WNN:Epoch 56: Training Loss 0.05273734009824693 	 Validation Loss 0.13005097210407257
INFO:WNN:Epoch 57: Training Loss 0.052296180200452604 	 Validation Loss 0.12994496524333954
INFO:WNN:Epoch 58: Training Loss 0.051857457224590085 	 Validation Loss 0.1299367994070053
INFO:WNN:Epoch 59: Training Loss 0.05142392800189555 	 Validation Loss 0.1300124228000641
INFO:WNN:Epoch 60: Training Loss 0.050992050052930914 	 Validation Loss 0.1301804780960083
INFO:WNN:Epoch 61: Training Loss 0.05056417748952905 	 Validation Loss 0.13044437766075134
threshold tensor(0.8987, grad_fn=<MulBackward0>)
