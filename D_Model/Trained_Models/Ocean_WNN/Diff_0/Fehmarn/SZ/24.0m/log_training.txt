INFO:WNN:Epoch 0: Training Loss 0.5595093885702747 	 Validation Loss 0.11441744118928909
INFO:WNN:Epoch 1: Training Loss 0.3662025488780013 	 Validation Loss 0.11600514501333237
INFO:WNN:Epoch 2: Training Loss 0.20695304258593492 	 Validation Loss 0.08509819768369198
INFO:WNN:Epoch 3: Training Loss 0.1143697492246117 	 Validation Loss 0.07240468822419643
INFO:WNN:Epoch 4: Training Loss 0.06641933691155698 	 Validation Loss 0.06813571602106094
INFO:WNN:Epoch 5: Training Loss 0.056459279265254736 	 Validation Loss 0.06526619754731655
INFO:WNN:Epoch 6: Training Loss 0.07846159268436688 	 Validation Loss 0.06263475865125656
INFO:WNN:Epoch 7: Training Loss 0.0915437286852726 	 Validation Loss 0.08206330612301826
INFO:WNN:Epoch 8: Training Loss 0.13758836324060603 	 Validation Loss 0.10545876622200012
INFO:WNN:Epoch 9: Training Loss 0.17981481266074947 	 Validation Loss 0.15684574097394943
INFO:WNN:Epoch 10: Training Loss 0.14469561981968582 	 Validation Loss 0.07877638749778271
INFO:WNN:Epoch 11: Training Loss 0.12355750020859498 	 Validation Loss 0.09286826476454735
INFO:WNN:Epoch 12: Training Loss 0.06406620592211507 	 Validation Loss 0.14092833176255226
INFO:WNN:Epoch 13: Training Loss 0.06725528653311942 	 Validation Loss 0.18294518440961838
INFO:WNN:Epoch 14: Training Loss 0.08532790961076639 	 Validation Loss 0.16951147839426994
INFO:WNN:Epoch 15: Training Loss 0.10380265828488129 	 Validation Loss 0.097710520029068
INFO:WNN:Epoch 16: Training Loss 0.07807443360798061 	 Validation Loss 0.06421409361064434
INFO:WNN:Epoch 17: Training Loss 0.06414600312876116 	 Validation Loss 0.07448607683181763
threshold tensor(0.9624, grad_fn=<MulBackward0>)
