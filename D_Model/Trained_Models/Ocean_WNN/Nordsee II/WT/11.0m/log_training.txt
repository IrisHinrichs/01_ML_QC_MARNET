INFO:WNN:Epoch 0: Training Loss 0.03597034433773989 	 Validation Loss 0.059722453355789185
INFO:WNN:Epoch 1: Training Loss 0.002498119218451452 	 Validation Loss 0.052784659899771214
INFO:WNN:Epoch 2: Training Loss 0.002660178917839432 	 Validation Loss 0.04869006620720029
INFO:WNN:Epoch 3: Training Loss 0.0026083707050401565 	 Validation Loss 0.04581126617267728
INFO:WNN:Epoch 4: Training Loss 0.0025709197748989437 	 Validation Loss 0.04249296709895134
INFO:WNN:Epoch 5: Training Loss 0.002696798196167595 	 Validation Loss 0.03892725007608533
INFO:WNN:Epoch 6: Training Loss 0.0029195703175519745 	 Validation Loss 0.03541259653866291
INFO:WNN:Epoch 7: Training Loss 0.003193643553282938 	 Validation Loss 0.03309514094144106
INFO:WNN:Epoch 8: Training Loss 0.0034654118974891215 	 Validation Loss 0.03248232323676348
INFO:WNN:Epoch 9: Training Loss 0.003463063027612084 	 Validation Loss 0.027123634237796068
INFO:WNN:Epoch 10: Training Loss 0.003189778424242714 	 Validation Loss 0.023806439712643623
INFO:WNN:Epoch 11: Training Loss 0.0035831414236106923 	 Validation Loss 0.027312524616718292
INFO:WNN:Epoch 12: Training Loss 0.00415243745608736 	 Validation Loss 0.03366905823349953
INFO:WNN:Epoch 13: Training Loss 0.004808795270477158 	 Validation Loss 0.04565166449174285
INFO:WNN:Epoch 14: Training Loss 0.004261233865338821 	 Validation Loss 0.04380744835361838
INFO:WNN:Epoch 15: Training Loss 0.004639150431865992 	 Validation Loss 0.04885764513164759
INFO:WNN:Epoch 16: Training Loss 0.0039822526176125864 	 Validation Loss 0.04452653136104345
INFO:WNN:Epoch 17: Training Loss 0.003408265956808933 	 Validation Loss 0.0483915526419878
INFO:WNN:Epoch 18: Training Loss 0.004140667442698032 	 Validation Loss 0.053705227095633745
INFO:WNN:Epoch 19: Training Loss 0.004304349580178818 	 Validation Loss 0.052646106109023094
INFO:WNN:Epoch 20: Training Loss 0.004054983433302221 	 Validation Loss 0.055941530503332615
INFO:WNN:Epoch 21: Training Loss 0.004081414546817541 	 Validation Loss 0.05019773310050368
threshold tensor(0.2472, grad_fn=<MulBackward0>)
