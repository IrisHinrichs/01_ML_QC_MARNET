INFO:WNN:Epoch 0: Training Loss 0.025343446778566207 	 Validation Loss 0.0005587924720852502
INFO:WNN:Epoch 1: Training Loss 0.0024996981738107916 	 Validation Loss 0.0007825889790962849
INFO:WNN:Epoch 2: Training Loss 0.0019384716684863127 	 Validation Loss 0.0017823637795767614
INFO:WNN:Epoch 3: Training Loss 0.0016668798545281715 	 Validation Loss 0.0024594811090667334
INFO:WNN:Epoch 4: Training Loss 0.0015024821559644348 	 Validation Loss 0.0019939086632803082
INFO:WNN:Epoch 5: Training Loss 0.0014508172012471484 	 Validation Loss 0.0010586515917176647
INFO:WNN:Epoch 6: Training Loss 0.001453215038521443 	 Validation Loss 0.000683107554712998
INFO:WNN:Epoch 7: Training Loss 0.0013334008496302587 	 Validation Loss 0.0005064846087147348
INFO:WNN:Epoch 8: Training Loss 0.0015415152369137387 	 Validation Loss 0.0009642337848033224
INFO:WNN:Epoch 9: Training Loss 0.0015664094245417736 	 Validation Loss 0.0007086015331359315
INFO:WNN:Epoch 10: Training Loss 0.0016636200208887582 	 Validation Loss 0.0011420656561053225
INFO:WNN:Epoch 11: Training Loss 0.001565633705081583 	 Validation Loss 0.0009627971199474164
INFO:WNN:Epoch 12: Training Loss 0.0016488638113385974 	 Validation Loss 0.0009722397163776415
INFO:WNN:Epoch 13: Training Loss 0.0016383521162215322 	 Validation Loss 0.0009586916421540082
INFO:WNN:Epoch 14: Training Loss 0.001551156136505661 	 Validation Loss 0.0005895457870792598
INFO:WNN:Epoch 15: Training Loss 0.001564489043767428 	 Validation Loss 0.0006089471155844096
INFO:WNN:Epoch 16: Training Loss 0.0013035070685519367 	 Validation Loss 0.0007256532096237477
INFO:WNN:Epoch 17: Training Loss 0.001467990873244862 	 Validation Loss 0.0007800550457821893
INFO:WNN:Epoch 18: Training Loss 0.001317179524811063 	 Validation Loss 0.0003050768739610378
INFO:WNN:Epoch 19: Training Loss 0.001119763432111178 	 Validation Loss 0.0006883011166272419
INFO:WNN:Epoch 20: Training Loss 0.0012843043603795268 	 Validation Loss 0.0005377523817255028
INFO:WNN:Epoch 21: Training Loss 0.0012132940178443945 	 Validation Loss 0.00043719689710997045
INFO:WNN:Epoch 22: Training Loss 0.0010025739710590239 	 Validation Loss 0.0003080894052150792
INFO:WNN:Epoch 23: Training Loss 0.0010122725410620717 	 Validation Loss 0.00040450932075535614
INFO:WNN:Epoch 24: Training Loss 0.0009206073744892425 	 Validation Loss 0.00044641077485201616
INFO:WNN:Epoch 25: Training Loss 0.0010986112213332143 	 Validation Loss 0.00038986728551598
INFO:WNN:Epoch 26: Training Loss 0.0008455205448441538 	 Validation Loss 0.0003067487590929626
INFO:WNN:Epoch 27: Training Loss 0.0008213301169444175 	 Validation Loss 0.00029332968032187115
INFO:WNN:Epoch 28: Training Loss 0.0008581968615415833 	 Validation Loss 0.00026807056357418854
INFO:WNN:Epoch 29: Training Loss 0.0008510570812169272 	 Validation Loss 0.00034973612595682165
INFO:WNN:Epoch 30: Training Loss 0.0006576044511308057 	 Validation Loss 0.00030375451855694076
INFO:WNN:Epoch 31: Training Loss 0.0007261716531083787 	 Validation Loss 0.0003305840987845191
INFO:WNN:Epoch 32: Training Loss 0.0006745379159958923 	 Validation Loss 0.000307861107493019
INFO:WNN:Epoch 33: Training Loss 0.0006446728285940867 	 Validation Loss 0.00032999512352814363
INFO:WNN:Epoch 34: Training Loss 0.0006686583316306207 	 Validation Loss 0.00037472659356093833
INFO:WNN:Epoch 35: Training Loss 0.0006237517217438425 	 Validation Loss 0.00030008340192060653
INFO:WNN:Epoch 36: Training Loss 0.0006410574546983065 	 Validation Loss 0.0003801389310475705
INFO:WNN:Epoch 37: Training Loss 0.0005896082811122407 	 Validation Loss 0.0002706308857471283
INFO:WNN:Epoch 38: Training Loss 0.0006125875936323505 	 Validation Loss 0.0003820049376892192
INFO:WNN:Epoch 39: Training Loss 0.0005770040319862145 	 Validation Loss 0.0002954396913992241
threshold tensor(0.0085, grad_fn=<MulBackward0>)
