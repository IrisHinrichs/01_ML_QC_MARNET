INFO:WNN:Epoch 0: Training Loss 0.641632700455375 	 Validation Loss 1.6256743669509888
INFO:WNN:Epoch 1: Training Loss 0.5526728313416243 	 Validation Loss 1.523197017610073
INFO:WNN:Epoch 2: Training Loss 0.4826005295617506 	 Validation Loss 1.4548939764499664
INFO:WNN:Epoch 3: Training Loss 0.4357177108176984 	 Validation Loss 1.4232952073216438
INFO:WNN:Epoch 4: Training Loss 0.4049102714246449 	 Validation Loss 1.43759136646986
INFO:WNN:Epoch 5: Training Loss 0.3687927981760974 	 Validation Loss 1.4427816718816757
INFO:WNN:Epoch 6: Training Loss 0.3662822706780086 	 Validation Loss 1.4677273482084274
INFO:WNN:Epoch 7: Training Loss 0.35308872229264426 	 Validation Loss 1.4796013459563255
INFO:WNN:Epoch 8: Training Loss 0.33085299977877486 	 Validation Loss 1.4909978955984116
INFO:WNN:Epoch 9: Training Loss 0.31701319536659867 	 Validation Loss 1.5260320007801056
INFO:WNN:Epoch 10: Training Loss 0.33457647217437625 	 Validation Loss 1.587155744433403
INFO:WNN:Epoch 11: Training Loss 0.2891644014744088 	 Validation Loss 1.503907822072506
INFO:WNN:Epoch 12: Training Loss 0.3028815136834358 	 Validation Loss 1.446854904294014
INFO:WNN:Epoch 13: Training Loss 0.2834661528468132 	 Validation Loss 1.5004413053393364
INFO:WNN:Epoch 14: Training Loss 0.27641428785864264 	 Validation Loss 1.473026379942894
threshold tensor(39.5718, grad_fn=<MulBackward0>)
