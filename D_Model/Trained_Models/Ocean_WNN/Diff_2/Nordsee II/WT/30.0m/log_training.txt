INFO:WNN:Epoch 0: Training Loss 0.36381713999435306 	 Validation Loss 2.219604078680277
INFO:WNN:Epoch 1: Training Loss 0.2780101411238623 	 Validation Loss 2.0918128974735737
INFO:WNN:Epoch 2: Training Loss 0.225301484654968 	 Validation Loss 1.971667792648077
INFO:WNN:Epoch 3: Training Loss 0.21631771275618425 	 Validation Loss 1.9287121444940567
INFO:WNN:Epoch 4: Training Loss 0.200367001137541 	 Validation Loss 1.8819829821586609
INFO:WNN:Epoch 5: Training Loss 0.18513639752442637 	 Validation Loss 1.899631280452013
INFO:WNN:Epoch 6: Training Loss 0.18265343156720823 	 Validation Loss 1.8609653636813164
INFO:WNN:Epoch 7: Training Loss 0.18219160822142536 	 Validation Loss 1.8192777298390865
INFO:WNN:Epoch 8: Training Loss 0.1705784179503098 	 Validation Loss 1.771114319562912
INFO:WNN:Epoch 9: Training Loss 0.16344456318377829 	 Validation Loss 1.7956942319869995
INFO:WNN:Epoch 10: Training Loss 0.15876347140874714 	 Validation Loss 1.7743710577487946
INFO:WNN:Epoch 11: Training Loss 0.15614708161835247 	 Validation Loss 1.765455450862646
INFO:WNN:Epoch 12: Training Loss 0.15232921281130984 	 Validation Loss 1.8164303079247475
INFO:WNN:Epoch 13: Training Loss 0.154186587043417 	 Validation Loss 1.8432853445410728
INFO:WNN:Epoch 14: Training Loss 0.1503379566129297 	 Validation Loss 1.8584034852683544
INFO:WNN:Epoch 15: Training Loss 0.14621530969937643 	 Validation Loss 1.87471367046237
INFO:WNN:Epoch 16: Training Loss 0.14582557037162283 	 Validation Loss 1.881877575069666
INFO:WNN:Epoch 17: Training Loss 0.14942483068443835 	 Validation Loss 1.8879508562386036
INFO:WNN:Epoch 18: Training Loss 0.142923171146928 	 Validation Loss 1.8986092582345009
INFO:WNN:Epoch 19: Training Loss 0.13442249345825985 	 Validation Loss 1.8978452906012535
INFO:WNN:Epoch 20: Training Loss 0.13225024121735865 	 Validation Loss 1.9499008990824223
INFO:WNN:Epoch 21: Training Loss 0.13685519413168853 	 Validation Loss 1.9639952443540096
INFO:WNN:Epoch 22: Training Loss 0.1334597749131111 	 Validation Loss 1.95396151766181
threshold tensor(68.1371, grad_fn=<MulBackward0>)
