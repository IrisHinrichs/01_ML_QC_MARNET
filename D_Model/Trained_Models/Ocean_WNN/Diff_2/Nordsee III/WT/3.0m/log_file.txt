INFO:WNN:Epoch 0: Training Loss 0.4265325923760732 	 Validation Loss 2.8315157890319824
INFO:WNN:Epoch 1: Training Loss 0.32892558773358666 	 Validation Loss 2.5743658542633057
INFO:WNN:Epoch 2: Training Loss 0.29080351889133454 	 Validation Loss 2.395244598388672
INFO:WNN:Epoch 3: Training Loss 0.26582030256589256 	 Validation Loss 2.338172435760498
INFO:WNN:Epoch 4: Training Loss 0.25145988911390305 	 Validation Loss 2.3023252487182617
INFO:WNN:Epoch 5: Training Loss 0.23911984165509542 	 Validation Loss 2.2950100898742676
INFO:WNN:Epoch 6: Training Loss 0.2258436252673467 	 Validation Loss 2.3006246089935303
INFO:WNN:Epoch 7: Training Loss 0.21461535394191741 	 Validation Loss 2.3068413734436035
INFO:WNN:Epoch 8: Training Loss 0.20067530522743862 	 Validation Loss 2.3250465393066406
INFO:WNN:Epoch 9: Training Loss 0.18792564223210018 	 Validation Loss 2.3489859104156494
INFO:WNN:Epoch 10: Training Loss 0.17575948536396027 	 Validation Loss 2.3765525817871094
INFO:WNN:Epoch 11: Training Loss 0.1650647610425949 	 Validation Loss 2.4031641483306885
INFO:WNN:Epoch 12: Training Loss 0.15543688436349232 	 Validation Loss 2.426948308944702
INFO:WNN:Epoch 13: Training Loss 0.14686683664719263 	 Validation Loss 2.4465105533599854
threshold tensor(31.8331, grad_fn=<MulBackward0>)
